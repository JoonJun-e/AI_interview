import { SpeechClient } from '@google-cloud/speech';
import { VertexAI } from '@google-cloud/vertexai';
import { Storage } from '@google-cloud/storage';
import { google } from 'googleapis';

// --- Google Cloud ì„¤ì • ---
const credentials = JSON.parse(process.env.GCP_CREDENTIALS);
const GCS_BUCKET_NAME = 'YOUR_BUCKET_NAME'; // ğŸ‘ˆ ì—¬ê¸°ì— ìƒì„±í•œ GCS ë²„í‚· ì´ë¦„ì„ ë„£ìœ¼ì„¸ìš”.
const GOOGLE_SHEET_ID = 'YOUR_SHEET_ID';   // ğŸ‘ˆ ì—¬ê¸°ì— êµ¬ê¸€ ì‹œíŠ¸ IDë¥¼ ë„£ìœ¼ì„¸ìš”.

const speechClient = new SpeechClient({ credentials });
const storage = new Storage({ credentials });
const vertexAI = new VertexAI({ project: credentials.project_id, location: 'us-central1' });

export default async function handler(req, res) {
    if (req.method !== 'POST') {
        return res.status(405).json({ error: 'Method Not Allowed' });
    }

    try {
        const { userInfo, answers } = req.body;
        const transcripts = [];
        const audioUrls = [];

        // ì „ë‹¬ë°›ì€ answers ë°°ì—´ì„ ìˆœíšŒí•˜ë©° ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        for (const answer of answers) {
            if (answer.type === 'video' && answer.content) {
                // ë¹„ë””ì˜¤ ë‹µë³€ ì²˜ë¦¬
                const audioBuffer = Buffer.from(answer.content, 'base64');
                const uniqueFileName = `${Date.now()}-${userInfo.name.replace(/\s/g, '')}.webm`;
                const gcsUri = `gs://${GCS_BUCKET_NAME}/${uniqueFileName}`;

                const [publicUrl, transcript] = await Promise.all([
                    uploadToGCS(audioBuffer, uniqueFileName),
                    speechToTextLong(gcsUri)
                ]);
                
                audioUrls.push(publicUrl);
                transcripts.push(transcript || "(ìŒì„± ì¸ì‹ ì‹¤íŒ¨)");

            } else if (answer.type === 'text') {
                // í…ìŠ¤íŠ¸ ë‹µë³€ ì²˜ë¦¬
                transcripts.push(`[ì½”ë”© í…ŒìŠ¤íŠ¸ ë‹µë³€]:\n${answer.content || "(ë‹µë³€ ì—†ìŒ)"}`);
                audioUrls.push("(í…ìŠ¤íŠ¸ ë‹µë³€)"); // ì‹œíŠ¸ì— í‘œì‹œë  ë‚´ìš©
            }
        }
        
        console.log(`Files uploaded:`, audioUrls);
        console.log(`STT Results:`, transcripts);

        const fullTranscript = transcripts.join('\n\n---\n\n');
        const geminiResult = await getGeminiResult(fullTranscript);
        console.log('Gemini Result:', geminiResult);

        const sheetRow = [
            userInfo.irb_consented_at,
            userInfo.name,
            userInfo.id, // 'age' ëŒ€ì‹  'id'ë¥¼ ì‹œíŠ¸ì— ê¸°ë¡
            audioUrls.join(', '),
            fullTranscript,
            geminiResult
        ];
        await appendToSheet(sheetRow);
        
        res.status(200).json({ result: geminiResult });

    } catch (error) {
        console.error('API Error:', error);
        res.status(500).json({ error: 'An error occurred.', details: error.message });
    }
}

async function uploadToGCS(buffer, fileName) {
    const bucket = storage.bucket(GCS_BUCKET_NAME);
    const file = bucket.file(fileName);
    await file.save(buffer, { metadata: { contentType: 'audio/webm' } });
    await file.makePublic(); 
    return file.publicUrl();
}

async function appendToSheet(rowData) {
    const auth = new google.auth.GoogleAuth({
        credentials,
        scopes: ['https://www.googleapis.com/auth/spreadsheets'],
    });
    const sheets = google.sheets({ version: 'v4', auth });
    await sheets.spreadsheets.values.append({
        spreadsheetId: GOOGLE_SHEET_ID,
        range: 'A1',
        valueInputOption: 'USER_ENTERED',
        resource: { values: [rowData] },
    });
}

async function speechToTextLong(gcsUri) {
    if (!gcsUri) return "";
    const audio = { uri: gcsUri };
    const config = {
        encoding: 'WEBM_OPUS',
        sampleRateHertz: 48000,
        languageCode: 'ko-KR',
    };
    const request = { audio, config };
    const [operation] = await speechClient.longRunningRecognize(request);
    const [response] = await operation.promise();
    return response.results.map(result => result.alternatives[0].transcript).join('\n');
}

async function getGeminiResult(text) {
    const generativeModel = vertexAI.getGenerativeModel({ model: 'gemini-1.0-pro' });
    const prompt = `ë‹¹ì‹ ì€ AI ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ìˆœì„œëŒ€ë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ 'í•©ê²©' ë˜ëŠ” 'ë¶ˆí•©ê²©'ìœ¼ë¡œë§Œ íŒë‹¨í•´ì£¼ì„¸ìš”. ì½”ë”© í…ŒìŠ¤íŠ¸ ë‹µë³€ì— ëŒ€í•´ì„œëŠ” ì •ë‹µ ì—¬ë¶€ë¥¼ ê°„ëµí•˜ê²Œ ì–¸ê¸‰í•˜ê³ , ë‚˜ë¨¸ì§€ ì¸ì„± ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ë¡ ì„ ë‚´ì£¼ì„¸ìš”.
    ---
    [ì§€ì›ì ë‹µë³€ ë‚´ìš©]
    ${text}
    ---
    `;
    const resp = await generativeModel.generateContent(prompt);
    return resp.response.candidates[0].content.parts[0].text;
}